{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch raw card data\n",
    "cardfile = \"raw/oracle-cards-20240201100133.json\"\n",
    "\n",
    "with open(cardfile, 'r', encoding='utf-8') as raw_oracle:\n",
    "    raw_oracle_data = json.load(raw_oracle)\n",
    "\n",
    "filtered_data = [card for card in raw_oracle_data if card.get('set_type') != \"memorabilia\"]\n",
    "\n",
    "columns_to_keep = [\"name\", \"mana_cost\", \"cmc\", \"type_line\", \"oracle_text\", \"power\", \"toughness\",\n",
    "                    \"colors\", \"color_identity\", \"keywords\"]\n",
    "\n",
    "df = pd.DataFrame(filtered_data)\n",
    "df = df[columns_to_keep]\n",
    "df.to_csv(\"raw/filtered_oracle_database.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d51255288e4c3cb6b6ad0f09bb5c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578ca83aaa2f4079aee1d7b00f35bba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649451f9f772410d8cf6163eade04f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c246b4d9435e4fa6b16a4f7fac55263e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token = \"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\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data using llama2 tokenizer\n",
    "\n",
    "formatted_rows = []\n",
    "for index, row in df.iterrows():\n",
    "    formatted_row = \"\"\n",
    "    for column_name, value in row.items():\n",
    "        formatted_row += f\"{column_name}: {value}\\n\"\n",
    "    formatted_rows.append(formatted_row.strip())\n",
    "    \n",
    "\n",
    "tokenized_rows = [tokenizer(row, return_tensors='tf', max_length=512, truncation=True) for row in formatted_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Data: 100%|██████████| 29244/29244 [00:16<00:00, 1795.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Chunk the data based on the max tokens for llama2\n",
    "\n",
    "max_token_limit = 1024\n",
    "\n",
    "def chunk_tokenized_data(tokenized_data):\n",
    "    tokenized_chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_tokens = 0\n",
    "\n",
    "    with tqdm(total=len(tokenized_data), desc=\"Chunking Data\") as pbar:\n",
    "        for tokenized_row in tokenized_data:\n",
    "            tokens_length = sum(len(tokenized_row[key][0]) for key in tokenized_row.keys())\n",
    "            if current_chunk_tokens + tokens_length <= max_token_limit:\n",
    "                current_chunk.append(tokenized_row)\n",
    "                current_chunk_tokens += tokens_length\n",
    "            else:\n",
    "                tokenized_chunks.append(current_chunk)\n",
    "                current_chunk = [tokenized_row]\n",
    "                current_chunk_tokens = tokens_length\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Add last chunk\n",
    "        if current_chunk:\n",
    "            tokenized_chunks.append(current_chunk)\n",
    "    return tokenized_chunks\n",
    "\n",
    "tokenized_chunks = chunk_tokenized_data(tokenized_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in the database successfully.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create SQLite database to store decoded tokenized data\n",
    "\n",
    "conn = sqlite3.connect('raw/tokenized_data_llama2.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS tokenized_chunks (\n",
    "                    chunk_id INTEGER PRIMARY KEY,\n",
    "                    chunk_text TEXT\n",
    "                )''')\n",
    "\n",
    "for i, chunk in enumerate(tokenized_chunks):\n",
    "    chunk_text = \"\"\n",
    "    for tokenized_row in chunk:\n",
    "        for key, value in tokenized_row.items():\n",
    "            chunk_text += tokenizer.decode(value[0]) + \"\\n\"\n",
    "    cursor.execute(\"INSERT INTO tokenized_chunks (chunk_id, chunk_text) VALUES (?, ?)\", (i, chunk_text))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data stored in the database successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Create test database\n",
    "\n",
    "cardfile = \"raw/filtered_oracle_database.csv\"\n",
    "df_oracle = pd.read_csv(cardfile)\n",
    "\n",
    "card_names = df_oracle[\"name\"].tolist()\n",
    "\n",
    "num_sample_cards = 1000\n",
    "\n",
    "random_card_names = random.sample(card_names, num_sample_cards)\n",
    "\n",
    "data = {'Card Name': random_card_names,\n",
    "        'GPT2-raw-ROUGE': [None] * num_sample_cards,  # Empty column for GPT-2-raw-ROUGE\n",
    "        'GPT2-RAG-ROUGE': [None] * num_sample_cards}  # Empty column for GPT-2-RAG-ROUGE\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#df.to_csv('RAG_MTG_Test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
